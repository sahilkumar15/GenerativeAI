 Lanyard regression is a statistical technique for modeling the relationship between an output variable and one or more input variables. In layman's terms, think of it as fitting a line through some data points as shown here, so you can make predictions on unknown data, assuming there is a linear relationship between the variables. You might be familiar with the linear function y equals mx plus b, where y is the output variable, also called the dependent variable. You may also see expressed as f of x, the function of the input variable. x on the other hand, would serve as the input variable, also called the independent variable. It's likely you'll see the coefficients m and b expressed as beta 1 and beta 0 respectively. So what do the m and b coefficients do? The m or beta 1 coefficient controls the slope of the line. The b or the beta 0 controls the intercept of the line. In machine learning, we also know it as the bias. These two coefficients are what we are solving for in linear regression. We can also extend to multiple input variables, so x1, x2, x3, with beta 1, beta 2, and beta 3, and so on, acting as slopes for each of those variables. In these higher dimensions, you would visualize the linear regression as a hyperplane. So how do we fit the line to these points? Well, you'll notice that there's these differences between the points and the line, these little red segments, these are called residuals. They are the differences between the data points and the predictions the line would produce. Take each of these residuals and square them. These are the squared errors, and notice that the large of the residuals are, the more amplified area of the squares are. If we total the areas of all of these squares for a given line, we will get the sum of the squared error, and this is known as our loss function. We need to find the beta 0 and beta 1 coefficients that will minimize that sum of squared error. The coefficients can be solved with a variety of techniques ranging from matrix decomposition to gradient descent, which is depicted right here. Thankfully, a lot of libraries are available to do this for us, and we will deep dive into these topics in other videos. To validate a linear regression, there are a number of techniques. Machine learning practitioners will often take a third of the data and put it into the test data set. The remaining two thirds will become the training data set. The training data set will then be used to fit the regression line. The test data set will then be used to validate the regression line. This is done to make sure that the regression performs well on data it has not seen before. The tricks used to evaluate the linear regression vary from the R square, standard error of the estimate, prediction intervals, as well as statistical significance. These are topics we will cover in future videos. If you enjoyed this video, please like and subscribe. Look at my two O'Reilly books, Essential Math for Data Science, and getting started with SQL. Chapter five of Essential Math for Data Science actually covers linear regression and much more depth. If you want live instruction, I also do teach on the O'Reilly platform. Promotional link below. I teach classes including machine learning from scratch, probability, and SQL. Comment on what topics you would like to see next, and I will see you again on 3 Minute Data Science.