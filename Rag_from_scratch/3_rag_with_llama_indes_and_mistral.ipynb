{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For creating RAG application\n",
    "\n",
    "1. DATA\n",
    "2. Embedding Model\n",
    "3. DATA BASE\n",
    "4. PROMPT\n",
    "5. LLM\n",
    "\n",
    "Connect all the things\n",
    "\n",
    "1. huggingface hub using hf token(inferencing)\n",
    "2. load the model in local memory from the hugging face(finetune)\n",
    "\n",
    "here lots of memory is requierd(7b,10b,50b,100b)\n",
    "\n",
    "bitandbytes\n",
    "\n",
    "\n",
    "3. GGML(GPT-Generated Model Language), GGUF(GPT-Generated Unified Format)\n",
    "\n",
    "4. OLAMAM,LLAMA-CPP,LM-STUDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "from llama_index.vector_stores.weaviate import WeaviateVectorStore\n",
    "from llama_index.core import StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the OPENAI_API_KEY\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# print(OPENAI_API_KEY)\n",
    "\n",
    "# Retrieve the WEAVIATE_API_KEY \n",
    "WEAVIATE_API_KEY = os.getenv(\"WEAVIATE_API_KEY\")\n",
    "# print(WEAVIATE_API_KEY)\n",
    "\n",
    "# Retrieve the WEAVIATE_CLUSTER link\n",
    "WEAVIATE_CLUSTER = os.getenv(\"WEAVIATE_CLUSTER\")\n",
    "# print(WEAVIATE_CLUSTER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = weaviate.Client(url= WEAVIATE_CLUSTER, auth_client_secret= weaviate.AuthApiKey(WEAVIATE_API_KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=SimpleDirectoryReader(r'D:\\iNeuron\\GenAI\\GenerativeAI\\Rag_from_scratch\\data')\n",
    "documents=loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2bb5f97f34c452cbe623ff24a71b84f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sahil\\.conda\\envs\\genai\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sahil\\AppData\\Local\\llama_index\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca5a47e322cf4909a39186f8776e0ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3610f41927474cb60b5cb48100f1f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e58773a7054543b2805fb6f0dbcc28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sahil\\.conda\\envs\\genai\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5e3fb837d9477cab50270add2331f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sahil\\.conda\\envs\\genai\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e315a2e36340ae8b6196e757846eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6203dfa5034be494e4f856a5dbda25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2bfcc860f114ff78789a3371ac8217a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a22c9e88b546899dbf5590f78c9c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a359fd6e75de47e68194e29b98ad5fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e61b8bad5440eaada0960b9eae21c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=\"\"\"\"<|SYSTEM|># You are a Q&A assistant. Your goal is to answer questions as\n",
    "accurately as possible based on the instructions and context provided\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will wrap the default prompts that are internal to llama-index\n",
    "query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. '''system & assistant(LLM)\n",
    "# 2. user'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_name: str = Field(\n",
    "        default=DEFAULT_HUGGINGFACE_MODEL,\n",
    "        description=(\n",
    "            \"The model name to use from HuggingFace. \"\n",
    "            \"Unused if `model` is passed in directly.\"\n",
    "        ),\n",
    "    )\n",
    "    context_window: int = Field(\n",
    "        default=DEFAULT_CONTEXT_WINDOW,\n",
    "        description=\"The maximum number of tokens available for input.\",\n",
    "        gt=0,\n",
    "    )\n",
    "    max_new_tokens: int = Field(\n",
    "        default=DEFAULT_NUM_OUTPUTS,\n",
    "        description=\"The maximum number of tokens to generate.\",\n",
    "        gt=0,\n",
    "    )\n",
    "    system_prompt: str = Field(\n",
    "        default=\"\",\n",
    "        description=(\n",
    "            \"The system prompt, containing any extra instructions or context. \"\n",
    "            \"The model card on HuggingFace should specify if this is needed.\"\n",
    "        ),\n",
    "    )\n",
    "    query_wrapper_prompt: PromptTemplate = Field(\n",
    "        default=PromptTemplate(\"{query_str}\"),\n",
    "        description=(\n",
    "            \"The query wrapper prompt, containing the query placeholder. \"\n",
    "            \"The model card on HuggingFace should specify if this is needed. \"\n",
    "            \"Should contain a `{query_str}` placeholder.\"\n",
    "        ),\n",
    "    )\n",
    "    tokenizer_name: str = Field(\n",
    "        default=DEFAULT_HUGGINGFACE_MODEL,\n",
    "        description=(\n",
    "            \"The name of the tokenizer to use from HuggingFace. \"\n",
    "            \"Unused if `tokenizer` is passed in directly.\"\n",
    "        ),\n",
    "    )\n",
    "    device_map: str = Field(\n",
    "        default=\"auto\", description=\"The device_map to use. Defaults to 'auto'.\"\n",
    "    )\n",
    "    stopping_ids: List[int] = Field(\n",
    "        default_factory=list,\n",
    "        description=(\n",
    "            \"The stopping ids to use. \"\n",
    "            \"Generation stops when these token IDs are predicted.\"\n",
    "        ),\n",
    "    )\n",
    "    tokenizer_outputs_to_remove: list = Field(\n",
    "        default_factory=list,\n",
    "        description=(\n",
    "            \"The outputs to remove from the tokenizer. \"\n",
    "            \"Sometimes huggingface tokenizers return extra inputs that cause errors.\"\n",
    "        ),\n",
    "    )\n",
    "    tokenizer_kwargs: dict = Field(\n",
    "        default_factory=dict, description=\"The kwargs to pass to the tokenizer.\"\n",
    "    )\n",
    "    model_kwargs: dict = Field(\n",
    "        default_factory=dict,\n",
    "        description=\"The kwargs to pass to the model during initialization.\",\n",
    "    )\n",
    "    generate_kwargs: dict = Field(\n",
    "        default_factory=dict,\n",
    "        description=\"The kwargs to pass to the model during generation.\",\n",
    "    )\n",
    "    is_chat_model: bool = Field(\n",
    "        default=False,\n",
    "        description=(\n",
    "            LLMMetadata.__fields__[\"is_chat_model\"].field_info.description\n",
    "            + \" Be sure to verify that you either pass an appropriate tokenizer \"\n",
    "            \"that can convert prompts to properly formatted chat messages or a \"\n",
    "            \"`messages_to_prompt` that does so.\"\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee84243c4c3a4d10a26a8dffca83793c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sahil\\.conda\\envs\\genai\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sahil\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e0e132c95648aa8ca829c9c56f9999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e107d1d107f4a858a6678a7750e5f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ae7d2d51d54b1dad5d225a08dfb7c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06a0f5bbb3f45e493e1c750ad0ac34a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d48fa2a3215047149ef81be7050b38c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd3fd41c06ca40f796372ffb8720d21c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9569ff79ff748b782f18c6b476a3237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd172260ae345e2a64ce8fe1e8127e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259c1c893dd74904a8d6eba274a1c6c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6149e16f0e4a4622bf3151f6dfe43ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "llm=HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": False},\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    device_map=\"auto\",\n",
    "    stopping_ids=[50278, 50279, 50277, 1, 0],\n",
    "    tokenizer_kwargs={\"max_length\": 4096},\n",
    "    # uncomment this if using CUDA to reduce memory usage\n",
    "    model_kwargs={\"torch_dtype\": torch.float16}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahil\\AppData\\Local\\Temp\\ipykernel_40964\\878835869.py:3: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  Service_Context=ServiceContext.from_defaults(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "Service_Context=ServiceContext.from_defaults(\n",
    "    chunk_size=1024,\n",
    "    llm=llm,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents=documents, service_context=Service_Context)\n",
    "query_engine =  index.as_query_engine(k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sahil\\.conda\\envs\\genai\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'We can use the matrix given in Figure 3 to evaluate the machine translation task. This matrix represents the weights, αij, of the annotation of the j-th source word for the i-th translated word. The matrix can be used to investigate the soft alignment between the translated sentence from the model and the input sentence. The matrix can also be used to visualize and see which word from the input sentence were considered more important for generating the target word. Additionally, we can use the BLEU score to evaluate the machine translation task. The BLEU score is a widely used metric for evaluating machine translation models. It measures the similarity between the generated text and a set of human reference texts. The BLEU score is calculated based on the number of n-grams (unigrams, bigrams, trigrams, etc.) that the generated text shares with the reference texts. The higher the BLEU score, the better the machine translation model.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"which matrics we can use for evaluating the machine translation task?\"\n",
    "result = query_engine.query(question)\n",
    "result.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='We can use the matrix given in Figure 3 to evaluate the machine translation task. This matrix represents the weights, αij, of the annotation of the j-th source word for the i-th translated word. The matrix can be used to investigate the soft alignment between the translated sentence from the model and the input sentence. The matrix can also be used to visualize and see which word from the input sentence were considered more important for generating the target word. Additionally, we can use the BLEU score to evaluate the machine translation task. The BLEU score is a widely used metric for evaluating machine translation models. It measures the similarity between the generated text and a set of human reference texts. The BLEU score is calculated based on the number of n-grams (unigrams, bigrams, trigrams, etc.) that the generated text shares with the reference texts. The higher the BLEU score, the better the machine translation model.', source_nodes=[NodeWithScore(node=TextNode(id_='16177827-dfb9-4fd9-8edc-af5cbcd93b1f', embedding=None, metadata={'page_label': '8', 'file_name': 'MachineTranslationwithAttention.pdf', 'file_path': 'D:\\\\iNeuron\\\\GenAI\\\\GenerativeAI\\\\Rag_from_scratch\\\\data\\\\MachineTranslationwithAttention.pdf', 'file_type': 'application/pdf', 'file_size': 482139, 'creation_date': '2024-05-26', 'last_modified_date': '2024-05-26'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e9a56be1-175d-437f-ac75-0a3bdda69bc2', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '8', 'file_name': 'MachineTranslationwithAttention.pdf', 'file_path': 'D:\\\\iNeuron\\\\GenAI\\\\GenerativeAI\\\\Rag_from_scratch\\\\data\\\\MachineTranslationwithAttention.pdf', 'file_type': 'application/pdf', 'file_size': 482139, 'creation_date': '2024-05-26', 'last_modified_date': '2024-05-26'}, hash='979d28ef36ec5b1c0e756283f327ef2664770b5ca24b3ff6a9fe5331396a60c1')}, text='(a)\\n (b)\\n(c)\\n (d)\\nFigure 3: Alignments translated from Spanish to English by our model. The row represents the translated\\nsentence, English and the column represents the source sentence, Spanish. Each of the cells of the matrix\\nrepresents weights, αij, of the annotation of the j-th source word for the i-th translated word. (1:White,\\n0:Black)', start_char_idx=0, end_char_idx=343, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5802055787444484), NodeWithScore(node=TextNode(id_='c2407e08-4dd5-41a3-8ff1-0021fc6718be', embedding=None, metadata={'page_label': '7', 'file_name': 'MachineTranslationwithAttention.pdf', 'file_path': 'D:\\\\iNeuron\\\\GenAI\\\\GenerativeAI\\\\Rag_from_scratch\\\\data\\\\MachineTranslationwithAttention.pdf', 'file_type': 'application/pdf', 'file_size': 482139, 'creation_date': '2024-05-26', 'last_modified_date': '2024-05-26'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='9d10d89e-b9fd-400c-a59e-200e7f99b6bb', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '7', 'file_name': 'MachineTranslationwithAttention.pdf', 'file_path': 'D:\\\\iNeuron\\\\GenAI\\\\GenerativeAI\\\\Rag_from_scratch\\\\data\\\\MachineTranslationwithAttention.pdf', 'file_type': 'application/pdf', 'file_size': 482139, 'creation_date': '2024-05-26', 'last_modified_date': '2024-05-26'}, hash='dd75ab6baa97e3baf736ab62614db70458e1b8945e542716ec3cef92cafc2fd9'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='95866416-a2fb-4792-b8d2-163b399eae85', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '7', 'file_name': 'MachineTranslationwithAttention.pdf', 'file_path': 'D:\\\\iNeuron\\\\GenAI\\\\GenerativeAI\\\\Rag_from_scratch\\\\data\\\\MachineTranslationwithAttention.pdf', 'file_type': 'application/pdf', 'file_size': 482139, 'creation_date': '2024-05-26', 'last_modified_date': '2024-05-26'}, hash='023e1eec6af955711b8fe724ef2137aac67a61c9611d58850471fa619f183442')}, text='This was our motivation to use the proposed ap-\\nproach by Bahdanau et al. (2014), where the per-\\nformance of the encoder-decoder with attention\\nshows no deterioration with sentence of length\\ngreater than 50 sentences. The result that we\\ngot which was 25.76 was quite close to the Bah-\\ndanau et al. (2014), where they got BLEU score of\\n26.75,training with 1000 encoder and decoder di-\\nmensions, and training on corpus of 384M words.\\nThey were also able to achieve BLEU score of\\n28.45 when trained the data until the performance\\nof the validation stopped improving.\\n6.2 Qualitative Results\\nThe model proposed by Bahdanau et al. (2014)\\nprovides a way to investigate the soft alignment\\nbetween the translated sentence from the model\\nand the input sentence. The matrix given in Fig\\n3, each of the cells represent the weights αijof\\nthe annotation of the j-th source word for the i-\\nth target word. This helps in visualizing and see\\nwhich word from the input sentence were con-\\nsidered more important for generating the target\\nword.\\nWe see that majority of the weights are con-\\ncentrated on the diagonal matrix, along with non-\\nmonotonic alignments. The non-monotonic align-\\nments would be high for long sentences, since the\\nwords in long target sentence tends to have depen-\\ndence on more than one word in source sentence.', start_char_idx=2954, end_char_idx=4270, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5380111684759608)], metadata={'16177827-dfb9-4fd9-8edc-af5cbcd93b1f': {'page_label': '8', 'file_name': 'MachineTranslationwithAttention.pdf', 'file_path': 'D:\\\\iNeuron\\\\GenAI\\\\GenerativeAI\\\\Rag_from_scratch\\\\data\\\\MachineTranslationwithAttention.pdf', 'file_type': 'application/pdf', 'file_size': 482139, 'creation_date': '2024-05-26', 'last_modified_date': '2024-05-26'}, 'c2407e08-4dd5-41a3-8ff1-0021fc6718be': {'page_label': '7', 'file_name': 'MachineTranslationwithAttention.pdf', 'file_path': 'D:\\\\iNeuron\\\\GenAI\\\\GenerativeAI\\\\Rag_from_scratch\\\\data\\\\MachineTranslationwithAttention.pdf', 'file_type': 'application/pdf', 'file_size': 482139, 'creation_date': '2024-05-26', 'last_modified_date': '2024-05-26'}})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can use the matrix given in Figure 3 to evaluate the machine translation task. This matrix represents the weights, αij, of the annotation of the j-th source word for the i-th translated word. The matrix can be used to investigate the soft alignment between the translated sentence from the model and the input sentence. The matrix can also be used to visualize and see which word from the input sentence were considered more important for generating the target word. Additionally, we can use the BLEU score to evaluate the machine translation task. The BLEU score is a widely used metric for evaluating machine translation models. It measures the similarity between the generated text and a set of human reference texts. The BLEU score is calculated based on the number of n-grams (unigrams, bigrams, trigrams, etc.) that the generated text shares with the reference texts. The higher the BLEU score, the better the machine translation model.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention is a mechanism used in machine translation to retain and utilize all the hidden state of the input sequence during the decoding phase. It allows the model to focus and pay more attention to the relevant part of the input sequence, which helps to improve the translation accuracy. The attention mechanism works by creating an alignment between each time step of the decoder output and all of the encoder hidden state. This alignment is learned during the training process, and each output of the decoder can selectively pick out specific elements from the sequence to produce the output. The attention mechanism helps to address the bottle-neck problem in RNN Encoder-Decoder models, where the complete sequence of information of the source sentence must be captured by one single vector, which is difficult for the decoder to summarize large input sequences at once.\n"
     ]
    }
   ],
   "source": [
    "result2=query_engine.query(\"what is attention mechnisam?\")\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Attention is a mechanism used in machine translation to retain and utilize all the hidden state of the input sequence during the decoding phase. It allows the model to focus and pay more attention to the relevant part of the input sequence, which helps to improve the translation accuracy. The attention mechanism works by creating an alignment between each time step of the decoder output and all of the encoder hidden state. This alignment is learned during the training process, and each output of the decoder can selectively pick out specific elements from the sequence to produce the output. The attention mechanism helps to address the bottle-neck problem in RNN Encoder-Decoder models, where the complete sequence of information of the source sentence must be captured by one single vector, which is difficult for the decoder to summarize large input sequences at once.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "# fixing unicode error in google colab\n",
    "import locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>We can use the matrix given in Figure 3 to evaluate the machine translation task. This matrix represents the weights, αij, of the annotation of the j-th source word for the i-th translated word. The matrix can be used to investigate the soft alignment between the translated sentence from the model and the input sentence. The matrix can also be used to visualize and see which word from the input sentence were considered more important for generating the target word. Additionally, we can use the BLEU score to evaluate the machine translation task. The BLEU score is a widely used metric for evaluating machine translation models. It measures the similarity between the generated text and a set of human reference texts. The BLEU score is calculated based on the number of n-grams (unigrams, bigrams, trigrams, etc.) that the generated text shares with the reference texts. The higher the BLEU score, the better the machine translation model.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{result}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# This is using Weaviate cloud which support linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Client' object has no attribute 'collections'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m client\u001b[38;5;241m=\u001b[39mweaviate\u001b[38;5;241m.\u001b[39mClient(url\u001b[38;5;241m=\u001b[39mWEAVIATE_CLUSTER,auth_client_secret\u001b[38;5;241m=\u001b[39mweaviate\u001b[38;5;241m.\u001b[39mAuthApiKey(WEAVIATE_API_KEY))\n\u001b[1;32m----> 2\u001b[0m vector_store\u001b[38;5;241m=\u001b[39m\u001b[43mWeaviateVectorStore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweaviate_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43mindex_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLlamaindex\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m storage_context \u001b[38;5;241m=\u001b[39m StorageContext\u001b[38;5;241m.\u001b[39mfrom_defaults(vector_store\u001b[38;5;241m=\u001b[39mvector_store)\n",
      "File \u001b[1;32mc:\\Users\\sahil\\.conda\\envs\\genai\\Lib\\site-packages\\llama_index\\vector_stores\\weaviate\\base.py:172\u001b[0m, in \u001b[0;36mWeaviateVectorStore.__init__\u001b[1;34m(self, weaviate_client, class_prefix, index_name, text_key, auth_config, client_kwargs, url, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    168\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex name must start with a capital letter, e.g. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLlamaIndex\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    169\u001b[0m     )\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# create default schema if does not exist\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mclass_schema_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    173\u001b[0m     create_default_schema(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client, index_name)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    176\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    177\u001b[0m     index_name\u001b[38;5;241m=\u001b[39mindex_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    180\u001b[0m     client_kwargs\u001b[38;5;241m=\u001b[39mclient_kwargs \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[0;32m    181\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\sahil\\.conda\\envs\\genai\\Lib\\site-packages\\llama_index\\vector_stores\\weaviate\\utils.py:76\u001b[0m, in \u001b[0;36mclass_schema_exists\u001b[1;34m(client, class_name)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check if class schema exists.\"\"\"\u001b[39;00m\n\u001b[0;32m     75\u001b[0m validate_client(client)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollections\u001b[49m\u001b[38;5;241m.\u001b[39mexists(class_name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Client' object has no attribute 'collections'"
     ]
    }
   ],
   "source": [
    "client=weaviate.Client(url=WEAVIATE_CLUSTER,auth_client_secret=weaviate.AuthApiKey(WEAVIATE_API_KEY))\n",
    "vector_store=WeaviateVectorStore(weaviate_client=client,index_name=\"Llamaindex\")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'storage_context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m index \u001b[38;5;241m=\u001b[39m VectorStoreIndex\u001b[38;5;241m.\u001b[39mfrom_documents(\n\u001b[1;32m----> 2\u001b[0m     documents, storage_context\u001b[38;5;241m=\u001b[39m\u001b[43mstorage_context\u001b[49m,embed_model\u001b[38;5;241m=\u001b[39membed_model\n\u001b[0;32m      3\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'storage_context' is not defined"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context,embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sahil\\.conda\\envs\\genai\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "result3=query_engine.query(\"what is attention?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Attention is a mechanism used in machine translation to retain and utilize all the hidden state of the input sentence during the decoding phase. It allows the model to focus and pay more attention to the relevant part of the input sequence. During the decoding phase, the model creates an alignment between each time step of the decoder output and all of the encoder hidden state. This allows the model to selectively pick out specific elements from the sequence to produce the output. The attention mechanism helps to memorize this information for longer sentences.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result3.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
